\documentclass[12pt]{report}

% кодировка входного файла
\usepackage[utf8]{inputenc}
% кодировка выходных символов
\usepackage[T1, T2A]{fontenc} 
% правила переноса слов, названия секций и другая локализация
\usepackage[english, russian]{babel}
% другое
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{dsfont}
\usepackage{caption}
\usepackage{svg}
\usepackage{float}
\usepackage{color}
\usepackage{amssymb}


\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\renewcommand{\thesection}{\arabic{section}}

\title{
ОТЧЕТ ПО ПРАКТИЧЕСКОЙ РАБОТЕ \\
<<Ансамбли алгоритмов. Веб-сервер.
Композиции алгоритмов для решения задачи регрессии.>>
}
\author{
    Тохчуков Данил Андреевич \\
    317 группа ВМК МГУ
}
\date{}

\begin{document}

\maketitle
\tableofcontents
\newpage

\section{Введение}
Основная цель данной работы - реализовать случайный лес и градиентный бустинг 
на базе деревьев и проанализировать как влияют 
параметры моделей на их качество. Заодно модели можно сравнить,
что мы и сделаем в этой работе. 
Исследования будем проводить на датасете данных о продажах недвижимости 
\href{https://www.kaggle.com/harlfoxem/housesalesprediction}{\textbf{House Sales in King County, USA}}
Параметры моделей, которые мы будем анализировать:
\begin{enumerate}
    \item количество деревьев в ансамблях -- \verb|n_estimators|
    \item размерность подвыборки признаков для дерева -- \verb|feature_subsample_size|
    \item максимальная глубина дерева -- \verb|max_depth|
    \item скорость обучения (только для градиентного бустинга) -- \verb|learning_rate|
\end{enumerate}

\noindentДалее требуется создать веб-сервер, презентующий реализованные модели.
Все исходные файлы будут в репозитории github: \href{https://github.com/makriot/Ensembles}{\textbf{репозиторий}}.


\section{Предобработка данных}
Данные представляют из себя csv-таблицу с 21 колонкой.
Требутся предсказать цену дома по его параметрам, записанным в колонки таблицы
Из этих колонок можно сразу исключить колонку ,,id'', ведь она не влияет на цену дома.
Интересная колонка: ,,date''. Возможно от даты цена на товары может сильно меняться, 
проверим это:

\begin{figure}[H]
    \begin{center}
        \includesvg[width=0.5\columnwidth]{date_price.svg}
    \end{center}
    \caption{Цена в уникальный день}\label{fig::1}
\end{figure}

\noindentОтлично! Периодов повышения или понижения цены нет, есть только 
скачки в некоторых датах -- удалим эту колонку (в каждую дату мы считали 
среднюю цену дома в этот день, всего в данных 372 уникальные даты). \\

\noindentДалее мы разобъём высю выборку на обучающую (0.8 от всей выборки) и отложенную
(0.2 от всей выборки).


\section{Случайный Лес}
Рассмотрим параметры для случайного леса, о которых мы говорили в введении.
Будем изучать метрику \textbf{RMSE} и \textbf{время работы алгоритма}
в зависимости от параметров.

\subsection{Количество деревьев в ансамбле}
Будем перебирать параметр в пределе 30 деревьев.
\begin{itemize}
    \item RMSE: \hyperref[fig::app1]{(Приложение 1)}
    \item Время обучения: \hyperref[fig::app2]{(Приложение 2)}
\end{itemize}

\noindentВидим, что при увеличении количества деревьев, качество улучшается, но 
при увеличении количества деревьев после достижения некоторого количества деревьев 
(для каждой сложности модели это количество своё)-- качество выходит на плато,
и уже не имеет смысла увеличивать количество деревьев. Время обучаения
закономерно увеличивается при увеличения количества деревьев, это связано с тем,
что мы делаем больше итераций в алгоритме. Отметим, что чем больше сложность деревьев,
тем дольше обучается модель. \\\\
Далее будем брать 20 деревьев в случайном лесе, чтобы точно выйти на плато для модели
любой сложности.

\subsection{Размерность подвыборки признаков для дерева}
\begin{itemize}
    \item RMSE: \hyperref[fig::app3]{(Приложение 3)}
    \item Время обучения: \hyperref[fig::app4]{(Приложение 4)}
\end{itemize}
Будем перебирать параметр \verb|feature_subsample_size|
RMSE убывает при небольших размерах признакового пространства (< 6), 
затем качество выходит на плато. То есть параметр $\lfloor \frac{n}{3}\rfloor$ = 6 
(у нас 18 признаков) в нашем случае оптимален (обычно таким его берут для задач регрессии)
Ну и при увеличении размерности признакового пространства закономерно увеличивается
время обучения модели, поэтому нужно брать параметр минимально возможным.

\subsection{Глубина}
\begin{itemize}
    \item RMSE: \hyperref[fig::app5]{(Приложение 5)}
    \item Время обучения: \hyperref[fig::app6]{(Приложение 6)}
\end{itemize}
RMSE случайного леса экспоненциально убывает с ростом сложности деревьев,
так и должно было быть, ведь чем сложнее модель тем больше зависимостей она может выявить.
Однако последней точкой нашего графика отрисовано качество модели, у которой сложность
деревьев не ограничена. Как мы видим, RMSE такой модели стало немного больше, 
чем у предыдущих моделей. Сложность модели стала слишком большой и переобучилась,
поэтому RMSE стало больше. Также отметим, что время обучения случайного леса с увеличением
сложности линейно увеличивается и достигает своего пика в модели с неограниченной сложностью.


\section{Градиентный Бустинг}
Теперь будем рассматривать параметры для Градиентного Бустинга. Также рассматриваем
метрику \textbf{RMSE} и \textbf{время работы алгоритма}.

\subsection{Количество деревьев в ансамбле}
Будем перебирать параметр в пределе 1000 деревьев.
\begin{itemize}
    \item RMSE: \hyperref[fig::app7]{(Приложение 7)}
    \item Время обучения: \hyperref[fig::app8]{(Приложение 8)}
\end{itemize}
RMSE убывает экспоненциально, причём с большой скоростью, поэтому рядом приведён ещё
один график в логарифмических шкалах. Отсюда видно, что чем больше деревьев, тем больше качество
Однако время обучения модели линейно возрастает с ростом числа деревьев в модели, поэтому
стоит ограничиться небольшим числом деревьев, но так, чтобы модель была сравнима по качеству
с моделью с больших числом дереьев (то есть найти оптимальный параметр учитывая качество 
и время обучения). Такой точкой является 400 деревьев -- на графиках
она отмечена красной точкой -- после неё, если прибавить целых 600 деревьев, качество сильно
не поменяется, а время обучения почти 14 секунд. \\

Получается, что чем больше деревьев в градиентном бустинге, тем точнее он может настроиться
на обучающую выборку, но время обучения стремительно возрастает, поэтому нужно выбирать что важнее.


\subsection{Размерность подвыборки признаков для дерева}
\begin{itemize}
    \item RMSE: \hyperref[fig::app9]{(Приложение 9)}
    \item Время обучения: \hyperref[fig::app10]{(Приложение 10)}
\end{itemize}
Время обучения закономерно возрастает при увеличения размерности признакового пространства.
Качество же скачет. Вызвано это тем, что признаки по своей сути неоднородны, какие-то
сильнее влияют на таргет(предсказание), какие-то меньше. Но всё равно, все ,,провалы'' 
RMSE (на нескольких запусках) находятся рядом с $\lfloor \frac{n}{3}\rfloor$ == 6, 
поэтому дальше мы будем брать параметр именно таким


\subsection{Глубина}
\begin{itemize}
    \item RMSE: \hyperref[fig::app11]{(Приложение 11)}
    \item Время обучения: \hyperref[fig::app12]{(Приложение 12)}
\end{itemize}
Лучший depth оказался равным 3-м. Чтобы строить более качественную модель, 
нужны более простые модели, потому что так мы снижаем корреляцию между 
этими моделями, а значит мы уменьшаем разброс.
Время обучения также линейно увеличивается с ростом глубины деревьев.
На модели с неограниченной глубиной деревьев -- пик времени обучения. 


\subsection{Скорость обучения}
Будем подбирать лучший \verb|learning_rate|
\begin{itemize}
    \item RMSE on logspace: \hyperref[fig::app13]{(Приложение 13)}
    \item RMSE near the 0.1: \hyperref[fig::app14]{(Приложение 14)}
    \item Время обучения: \hyperref[fig::app15]{(Приложение 15)}
\end{itemize}
Сначала начнём подбирать \verb|learning_rate| значениями вида: $10^z, z \in \mathbb{Z}$.
Результат подбора на первом графике.
Получилось, что где-то рядом с 0.1 находится оптимум.
Дальше, на втором графике, мы уточняем эту оценку параметра.
Наиболее оптимальный парметр оказался равным 0.3 -- в этой точке наибольшее
качество, и рядом с ней меньше всего скачков качества.  \\
Время обучения, чисто теоретически, не должно сильно зависеть от \verb|learning_rate|
потому что этот параметр не влияет на число итераций, а на обучение решающих деревьев
влияет очень мало. Параметр \verb|learning_rate| > 1 не был учтён, потому что когда 
градиентный бустинг обучается, он уже подбирает оптимальный коэффициент для нового дерева,
чтобы добавить его в ансамбль, и если этот коэффициент увеличить по модулю, то мы каждый
раз будем (вероятнее всего) перепрыгивать оптимум, и нет никакой гарантии, что метод
с такими параметрами сойдётся. 

\section{Вывод}
Ансамблевые методы отлично дополняют базовые алгоритмы: случайный лес уменьшает разброс
базовых алгоритмов, а градиентный бустинг ещё и уменьшает сдвиг (bias).
В работе удалось найти оптимальные параметры для каждой модели, с помощью чего получилось
достигнуть качества RMSE = 140000. И Случайный Лес и Градиентный Бустинг хороши,
но эксперименты показали, что Градиентный бустинг добивается лучшего качества, причём
мы узнали, что чем больше мощностей у компьютера, тем большее качество можно достигнуть,
потмоу что на наших графиках, RMSE не достигла ,,плато''.

\section{Приложения}
\renewcommand\thefigure{\thesection.\arabic{figure}}  
\setcounter{figure}{0}  
\renewcommand{\figurename}{Прил.}

\begin{figure}[H]
    \begin{center}
        \includesvg[height=0.5\columnwidth]{ntree_metric.svg}
    \end{center}
    \caption{Случайный лес}\label{fig::app1}
\end{figure}

\begin{figure}[H]
    \begin{center}
        \includesvg[height=0.5\columnwidth]{ntree_time.svg}
    \end{center}
    \caption{Случайный Лес}\label{fig::app2}
\end{figure}

\begin{figure}[H]
    \begin{center}
        \includesvg[height=0.5\columnwidth]{feature_metric.svg}
    \end{center}
    \caption{Случайный Лес}\label{fig::app3}
\end{figure}

\begin{figure}[H]
    \begin{center}
        \includesvg[height=0.5\columnwidth]{feature_time.svg}
    \end{center}
    \caption{Случайный Лес}\label{fig::app4}
\end{figure}

\begin{figure}[H]
    \begin{center}
        \includesvg[height=0.5\columnwidth]{depth_metric.svg}
    \end{center}
    \caption{Случайный Лес}\label{fig::app5}
\end{figure}

\begin{figure}[H]
    \begin{center}
        \includesvg[height=0.5\columnwidth]{depth_time.svg}
    \end{center}
    \caption{Случайный Лес}\label{fig::app6}
\end{figure}

\begin{figure}[H]
    \begin{center}
        \includesvg[height=0.5\columnwidth]{gb_ntree_metric.svg}
    \end{center}
    \caption{Градиентный Бустинг}\label{fig::app7}
\end{figure}

\begin{figure}[H]
    \begin{center}
        \includesvg[height=0.5\columnwidth]{gb_ntree_time.svg}
    \end{center}
    \caption{Градиентный Бустинг}\label{fig::app8}
\end{figure}

\begin{figure}[H]
    \begin{center}
        \includesvg[height=0.5\columnwidth]{gb_feature_metric.svg}
    \end{center}
    \caption{Градиентный Бустинг}\label{fig::app9}
\end{figure}

\begin{figure}[H]
    \begin{center}
        \includesvg[height=0.5\columnwidth]{gb_feature_time.svg}
    \end{center}
    \caption{Градиентный Бустинг}\label{fig::app10}
\end{figure}

\begin{figure}[H]
    \begin{center}
        \includesvg[height=0.5\columnwidth]{gb_depth_metric.svg}
    \end{center}
    \caption{Градиентный Бустинг}\label{fig::app11}
\end{figure}

\begin{figure}[H]
    \begin{center}
        \includesvg[height=0.5\columnwidth]{gb_depth_time.svg}
    \end{center}
    \caption{Градиентный Бустинг}\label{fig::app12}
\end{figure}

\begin{figure}[H]
    \begin{center}
        \includesvg[height=0.5\columnwidth]{gb_lr_metric.svg}
    \end{center}
    \caption{Градиентный Бустинг}\label{fig::app13}
\end{figure}

\begin{figure}[H]
    \begin{center}
        \includesvg[height=0.5\columnwidth]{gb_lr_metric_target.svg}
    \end{center}
    \caption{Градиентный Бустинг}\label{fig::app14}
\end{figure}

\begin{figure}[H]
    \begin{center}
        \includesvg[height=0.5\columnwidth]{gb_lr_time.svg}
    \end{center}
    \caption{Градиентный Бустинг}\label{fig::app15}
\end{figure}


\end{document}
